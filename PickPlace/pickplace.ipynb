{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium-robotics==1.3.1 \n",
    "#!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class custom(gym.Wrapper):\n",
    "    def __init__(self,env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reset(self,**kwargs):\n",
    "        obs,info = super().reset(**kwargs)\n",
    "        self.env.unwrapped.data.qpos[0] = .3 # robot base x pos\n",
    "        self.env.unwrapped.data.qpos[1] = .5 # robot base y pos\n",
    "        # self.env.unwrapped.data.qpos[15]   # block's x pos\n",
    "        # self.env.unwrapped.data.qpos[16]   # block's y pos\n",
    "        self.env.unwrapped.data.qpos[17] = .4\n",
    "        return obs,info\n",
    "\n",
    "    def step(self,action):\n",
    "        return super().step(action)\n",
    "\n",
    "def process_obs(obs:dict):\n",
    "    observation = obs.get(\"observation\")[:9]\n",
    "    achieved_goal = obs.get(\"achieved_goal\")\n",
    "    desired_goal = obs.get(\"desired_goal\")\n",
    "    return np.append(observation,(achieved_goal,desired_goal))\n",
    "\n",
    "def make_env():\n",
    "    x = gym.make(\"FetchPickAndPlace-v3\",max_episode_steps=100)\n",
    "    x = custom(x)\n",
    "    return x\n",
    "\n",
    "@dataclass()\n",
    "class Hypers:\n",
    "    lr = 3e-4\n",
    "    action_dim = 4\n",
    "    obs_dim = 15\n",
    "    alpha = 0.2\n",
    "    warmup = 10\n",
    "    gamma = 0.99\n",
    "    tau = 5e-3\n",
    "\n",
    "hypers = Hypers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(hypers.obs_dim,128)\n",
    "        self.l2 = nn.Linear(128,128)\n",
    "        self.mean = nn.Linear(128,hypers.action_dim)\n",
    "        self.std = nn.Linear(128,hypers.action_dim)\n",
    "        self.optim = Adam(self.parameters(),lr=hypers.lr)\n",
    "        self.apply(self.weights_init)\n",
    "\n",
    "    def forward(self):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        mean = self.mean(x)\n",
    "        std = self.std(x).clamp(-20,2).exp()\n",
    "        dist = Normal(mean,std)\n",
    "        pretanh = dist.rsample()\n",
    "        action = F.tanh(pretanh)\n",
    "\n",
    "        log = dist.log_prob(pretanh)\n",
    "        log -= torch.log(1-action.pow(2) + 1e-6)\n",
    "        log = log.sum(1,True)\n",
    "        return action,log,mean\n",
    "        \n",
    "class q_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(hypers.obs_dim+hypers.action_dim,128)\n",
    "        self.l2 = nn.Linear(128,128)\n",
    "        self.l3 = nn.Linear(128,1)\n",
    "        self.optim = Adam(self.parameters(),lr=hypers.lr)\n",
    "        self.apply(self.weights_init)\n",
    "    \n",
    "    def forward(self,obs:Tensor,action:Tensor):\n",
    "        x = torch.cat(obs,action,dim=-1)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab860a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class collector:\n",
    "    def __init__(self,env,policy):\n",
    "        self.data = []\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.obs = process_obs(self.env.reset()[0])\n",
    "        self.epi_reward = 0\n",
    "        self.reward = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def add(self):\n",
    "        action,_,_ = self.policy(self.obs)\n",
    "        nx_state,reward,done,trunc,info = self.env.step(action)\n",
    "        self.data.append(\n",
    "            self.obs,nx_state,reward,done,action\n",
    "        )\n",
    "        self.reward+=reward\n",
    "        if done:\n",
    "            self.epi_reward = self.reward\n",
    "            self.obs = process_obs(self.env.reset()[0])\n",
    "            self.reward = 0\n",
    "        else:\n",
    "            self.obs = nx_state\n",
    "\n",
    "    def sample(self,batch):\n",
    "        output = random.sample(self.data,batch)\n",
    "        state,nx_state,reward,done,action = zip(*output)\n",
    "        return state,nx_state,reward,done,action\n",
    "\n",
    "    def util(self): \n",
    "        return self.epi_reward\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class main:\n",
    "    def __init__(self):\n",
    "        self.policy = policy()\n",
    "        self.q1 = q_network()\n",
    "        self.q2 = q_network()\n",
    "        self.q1_target = deepcopy(self.q1)\n",
    "        self.q2_target = deepcopy(self.q2)\n",
    "        self.q_optim = Adam(list(self.q1.parameters())+list(self.q2.parameters()),lr=hypers.lr)\n",
    "        self.alpha = hypers.alpha\n",
    "        self.buffer = collector()\n",
    "        self.writter = SummaryWriter(\"./\")\n",
    "\n",
    "    def train(self,start=False):\n",
    "        if start:\n",
    "            for n in range(int(5e5)):\n",
    "                self.buffer.add()\n",
    "                if len(self.buffer)>= 2_000: # hypers.warmup \n",
    "                    states,nx_state,reward,dones,action = self.buffer.sample()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        target_action,log_target_action,_ = self.policy(states)\n",
    "                        q1_target = self.q1_target(nx_state,target_action)\n",
    "                        q2_target = self.q2_target(nx_state,target_action)\n",
    "                        q_target = reward + (1-dones) * hypers.gamma * (torch.min(q1_target,q2_target) - hypers.alpha * log_target_action)\n",
    "                    q1 = self.q1(states,action) \n",
    "                    q2 = self.q2(states,action)\n",
    "                    q_loss = F.mse_loss(q1,q_target) + F.mse_loss(q2,q_target)\n",
    "                    self.q_optim.zero_grad()\n",
    "                    q_loss.backward()\n",
    "                    self.q_optim.step()\n",
    "\n",
    "                    p_action,log_p_action,_ = self.policy(states)\n",
    "                    policy_loss = ((hypers.alpha*log_p_action) - self.q1(states,p_action)).mean()\n",
    "                    self.policy.optim.zero_grad()\n",
    "                    policy_loss.backward()\n",
    "                    self.policy.optim.step()\n",
    "\n",
    "                    for q1_params,q1_target_parms in zip(self.q1.parameters(),self.q1_target.parameters()):\n",
    "                        q1_target_parms.data.copy_((q1_params*hypers.tau)+(1.0-hypers.tau)*q1_target_parms)\n",
    "                    for q2_params,q2_target_params in zip(self.q2.parameters(),self.q2_target.parameters()):\n",
    "                        q2_target_params.data.copy_((q2_params*hypers.tau)+(1.0-hypers.tau)*q2_target_params)\n",
    "                    \n",
    "                    self.writter.add_scalar(\"Main/epi reward\",self.buffer.util(),n)\n",
    "                    self.writter.add_scalar(\"Main/action variance\",action.var(),n)\n",
    "                    self.writter.add_scalar(\"Main/policy loss action variance\",p_action.var(),n)\n",
    "                    self.writter.add_scalar(\"Main/policy loss\",policy_loss,n)\n",
    "                    self.writter.add_scalar(\"Main/q loss\",q_loss,n)\n",
    "                    self.writter.flush()\n",
    "\n",
    "main().train(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
